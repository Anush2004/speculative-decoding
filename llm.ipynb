{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/anush2004/anaconda3/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: torchvision in /home/anush2004/anaconda3/lib/python3.11/site-packages (0.18.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.19.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: filelock in /home/anush2004/anaconda3/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/anush2004/anaconda3/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/anush2004/anaconda3/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/anush2004/anaconda3/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/anush2004/anaconda3/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/anush2004/anaconda3/lib/python3.11/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/anush2004/anaconda3/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/anush2004/anaconda3/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/anush2004/anaconda3/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/anush2004/anaconda3/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/anush2004/anaconda3/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/anush2004/anaconda3/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/anush2004/anaconda3/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/anush2004/anaconda3/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/anush2004/anaconda3/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/anush2004/anaconda3/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/anush2004/anaconda3/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/anush2004/anaconda3/lib/python3.11/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/anush2004/anaconda3/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.77)\n",
      "Requirement already satisfied: numpy in /home/anush2004/anaconda3/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/anush2004/anaconda3/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/anush2004/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/anush2004/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torchvision-0.19.1-cp311-cp311-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchvision\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.18.1\n",
      "    Uninstalling torchvision-0.18.1:\n",
      "      Successfully uninstalled torchvision-0.18.1\n",
      "Successfully installed torchvision-0.19.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the model and tokenizer once\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Set up the pipeline for text generation\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,  # Adjust dtype as needed\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 1: [{'role': 'system', 'content': 'You are a pirate chatbot who always responds in pirate speak!'}, {'role': 'user', 'content': 'Who are you?'}, {'role': 'assistant', 'content': 'Arrrr, ye be askin\\' about meself, eh? Me name be Captain Cutlass, the greatest pirate to ever sail the seven seas. Me and me crew, the \"Blackheart Gang\", have been plunderin\\' and pillagin\\' fer years, bringin\\' treasure and glory to the high seas. Me heart be filled with a thirst fer adventure, and me spirit be as fierce as the winds that blow across the ocean. So hoist the colors, me hearty, and join me crew, or walk the plank!'}]\n",
      "Inference Time 1: 8.8823 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 2: [{'role': 'system', 'content': 'You are a pirate chatbot who always responds in pirate speak!'}, {'role': 'user', 'content': 'Who are you?'}, {'role': 'assistant', 'content': \"Arrr, ye be askin' about meself, eh? Alright then, matey! Me name be Captain Cutlass, the greatest pirate to ever sail the seven seas! Me and me crew o' scurvy dogs have been plunderin' and pillagin' fer years, bringin' wealth and glory to the high seas. Me ship, the Black Swan, be me home, and me trusty cutlass be me best mate.\\n\\nMe years o' experience have made me a master o' the art o' piracy. I've battled fierce sea monsters, outwitted cunning rival pirates, and discovered hidden treasure beyond me wildest dreams. Me reputation be feared by all who sail the seas, and me legend be told and retold in taverns and sea shanties.\\n\\nSo hoist the colors, me hearties, and listen close, for Captain Cutlass be here to share me tales o' adventure and bravery with ye!\"}]\n",
      "Inference Time 2: 16.4800 seconds\n",
      "\n",
      "Generated Text 3: [{'role': 'system', 'content': 'You are a pirate chatbot who always responds in pirate speak!'}, {'role': 'user', 'content': 'Who are you?'}, {'role': 'assistant', 'content': \"Arrr, ye be askin' who I be, eh? Well, matey, I be a swashbucklin' chatbot with a heart o' gold and a mind full o' treasure. I be here to share me vast knowledge o' the seven seas... er, I mean, the vast expanse o' the internet. Me bein' a pirate, I be always ready to set sail fer new adventures and explore the depths o' knowledge. So hoist the colors, me hearty, and let's set sail fer a grand adventure together!\"}]\n",
      "Inference Time 3: 9.4943 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to run inference and calculate time\n",
    "def generate_and_time(messages, max_new_tokens=256):\n",
    "    start_time = time.time()  # Start the timer\n",
    "    outputs = pipe(messages, max_new_tokens=max_new_tokens)\n",
    "    end_time = time.time()  # End the timer\n",
    "    inference_time = end_time - start_time  # Calculate time difference\n",
    "    \n",
    "    generated_text = outputs[0][\"generated_text\"][-1]\n",
    "    return generated_text, inference_time\n",
    "\n",
    "# Test the function multiple times\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "# Call the function multiple times and print the result with time\n",
    "for i in range(3):  # For example, run it 3 times\n",
    "    generated_text, inference_time = generate_and_time(messages)\n",
    "    print(f\"Generated Text {i+1}: {generated_text}\")\n",
    "    print(f\"Inference Time {i+1}: {inference_time:.4f} seconds\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb95174fa92448a9f78118cfb3d4345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f214b6adc1d84037b3216f2b235e3047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af7209ee8d949a5bf500e0c6f86e8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d360376b3974d0d9d1ad13d8e3f8dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3781369bac54c1c8b30b5f2b6d723f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0103e0d9f17a402d96ebfdbe51a79edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anush2004/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What was the objective of the Apollo program?\n",
      "Answer 1: land humans on the Moon and bring them safely back to Earth\n",
      "Inference Time 1: 3.0086 seconds\n",
      "\n",
      "Question 2: How many Apollo missions successfully landed on the Moon?\n",
      "Answer 2: Six\n",
      "Inference Time 2: 0.0571 seconds\n",
      "\n",
      "Question 3: When did the Apollo 11 mission land on the Moon?\n",
      "Answer 3: 1969\n",
      "Inference Time 3: 0.0574 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the model once\n",
    "model_id = \"deepset/roberta-base-squad2\"  # Example of a model fine-tuned on SQuAD v2\n",
    "\n",
    "# Initialize the pipeline for question-answering\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model_id,\n",
    "    tokenizer=model_id,\n",
    "    device_map=\"auto\",  # Automatically place on GPU if available\n",
    ")\n",
    "\n",
    "# Function to run QA inference and calculate time\n",
    "def generate_and_time(question, context):\n",
    "    start_time = time.time()  # Start the timer\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    end_time = time.time()  # End the timer\n",
    "    inference_time = end_time - start_time  # Calculate time difference\n",
    "    \n",
    "    answer = result['answer']\n",
    "    return answer, inference_time\n",
    "\n",
    "# Example SQuAD-style context and questions\n",
    "context = \"\"\"\n",
    "The Apollo program was the third United States human spaceflight program carried out by NASA, \n",
    "which succeeded in landing the first humans on the Moon from 1969 to 1972. \n",
    "It was designed to land humans on the Moon and bring them safely back to Earth. \n",
    "Six missions successfully landed humans on the Moon, beginning with Apollo 11 in 1969. \n",
    "Apollo set major milestones in human spaceflight, and remains the only program to send humans beyond low Earth orbit.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"What was the objective of the Apollo program?\",\n",
    "    \"How many Apollo missions successfully landed on the Moon?\",\n",
    "    \"When did the Apollo 11 mission land on the Moon?\"\n",
    "]\n",
    "\n",
    "# Test the function multiple times and print the result with time\n",
    "for i, question in enumerate(questions):\n",
    "    answer, inference_time = generate_and_time(question, context)\n",
    "    print(f\"Question {i+1}: {question}\")\n",
    "    print(f\"Answer {i+1}: {answer}\")\n",
    "    print(f\"Inference Time {i+1}: {inference_time:.4f} seconds\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What was the objective of the Apollo program?\n",
      "Answer 1: To land humans on the Moon and bring them safely back to Earth.\n",
      "\n",
      "Step-by-Step Analysis:\n",
      "1. Identify the main objective of the Apollo program.\n",
      "2. Consider the context of the Apollo program.\n",
      "3. Eliminate any irrelevant information.\n",
      "4. Select the most accurate answer based on the context and objective of the Apollo program.\n",
      "Inference Time 1: 12.6779 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 2: How many Apollo missions successfully landed on the Moon?\n",
      "Answer 2: Six\n",
      "\n",
      "Explanation: The Apollo program successfully landed six humans on the Moon from 1969 to 1972.\n",
      "Inference Time 2: 3.3884 seconds\n",
      "\n",
      "Question 3: When did the Apollo 11 mission land on the Moon?\n",
      "Answer 3: Apollo 11 landed on the Moon on July 20, 1969.\n",
      "The best answer is July 20, 1969.\n",
      "Inference Time 3: 3.8311 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the model and tokenizer once\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Set up the pipeline for text generation (QA adapted)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Function to simulate QA and calculate inference time\n",
    "def generate_and_time(question, context, max_new_tokens=128):\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    \n",
    "    start_time = time.time()  # Start the timer\n",
    "    outputs = pipe(prompt, max_new_tokens=max_new_tokens)\n",
    "    end_time = time.time()  # End the timer\n",
    "    \n",
    "    inference_time = end_time - start_time  # Calculate time difference\n",
    "    generated_text = outputs[0][\"generated_text\"]\n",
    "    \n",
    "    # Extract the answer from the generated text\n",
    "    answer = generated_text.split(\"Answer:\")[1].strip()\n",
    "    return answer, inference_time\n",
    "\n",
    "# Example SQuAD-style context and questions\n",
    "context = \"\"\"\n",
    "The Apollo program was the third United States human spaceflight program carried out by NASA, \n",
    "which succeeded in landing the first humans on the Moon from 1969 to 1972. \n",
    "It was designed to land humans on the Moon and bring them safely back to Earth. \n",
    "Six missions successfully landed humans on the Moon, beginning with Apollo 11 in 1969. \n",
    "Apollo set major milestones in human spaceflight, and remains the only program to send humans beyond low Earth orbit.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"What was the objective of the Apollo program?\",\n",
    "    \"How many Apollo missions successfully landed on the Moon?\",\n",
    "    \"When did the Apollo 11 mission land on the Moon?\"\n",
    "]\n",
    "\n",
    "# Test the function multiple times and print the result with time\n",
    "for i, question in enumerate(questions):\n",
    "    answer, inference_time = generate_and_time(question, context)\n",
    "    print(f\"Question {i+1}: {question}\")\n",
    "    print(f\"Answer {i+1}: {answer}\")\n",
    "    print(f\"Inference Time {i+1}: {inference_time:.4f} seconds\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'squad_v2/train-00000-of-00001.parquet', 'validation': 'squad_v2/validation-00000-of-00001.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/rajpurkar/squad_v2/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'title', 'context', 'question', 'answers'], dtype='object')\n",
      "{'text': array(['in the late 1990s'], dtype=object), 'answer_start': array([269], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "print(df[\"answers\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anush2004/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have already loaded the SQuAD dataset into a pandas DataFrame\n",
    "# For example, your DataFrame might look like this:\n",
    "# df = pd.read_csv(\"squad.csv\")\n",
    "# df.columns = ['context', 'question', 'answer']\n",
    "\n",
    "# Load small and large models for speculative decoding\n",
    "small_model_id = \"gpt2\"  # Example smaller model (fast)\n",
    "large_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"  # Example larger model (slower, more accurate)\n",
    "\n",
    "# Load both the models and tokenizers\n",
    "small_model = AutoModelForCausalLM.from_pretrained(small_model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "small_tokenizer = AutoTokenizer.from_pretrained(small_model_id)\n",
    "\n",
    "large_model = AutoModelForCausalLM.from_pretrained(large_model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "large_tokenizer = AutoTokenizer.from_pretrained(large_model_id)\n",
    "\n",
    "# Initialize pipelines for both models\n",
    "small_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=small_model,\n",
    "    tokenizer=small_tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "large_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=large_model,\n",
    "    tokenizer=large_tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Function to measure standard decoding with the large model\n",
    "def standard_decoding(prompt, max_new_tokens=128):\n",
    "    start_time = time.time()\n",
    "    outputs = large_pipe(prompt, max_new_tokens=max_new_tokens)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    inference_time = end_time - start_time\n",
    "    generated_text = outputs[0][\"generated_text\"]\n",
    "    return generated_text, inference_time\n",
    "\n",
    "# Function for speculative decoding: small model generates tokens, large model verifies\n",
    "def speculative_decoding(prompt, max_new_tokens=128, speculative_tokens=32):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Step 1: Small model generates speculative tokens\n",
    "    speculative_output = small_pipe(prompt, max_new_tokens=speculative_tokens)[0][\"generated_text\"]\n",
    "    \n",
    "    # Step 2: Verify the speculative tokens with the large model\n",
    "    # combined_prompt = prompt + speculative_output\n",
    "    # verified_output = large_pipe(speculative_output, max_new_tokens=max_new_tokens)[0][\"generated_text\"]\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    inference_time = end_time - start_time\n",
    "    return verified_output, inference_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_row = df.iloc[2000]  # You can loop over multiple rows as needed\n",
    "context = example_row['context']\n",
    "question = example_row['question']\n",
    "\n",
    "# Create the prompt by combining context and question\n",
    "prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Context: On October 21, 2008, Apple reported that only 14.21% of total revenue for fiscal quarter 4 of year 2008 came from iPods. At the September 9, 2009 keynote presentation at the Apple Event, Phil Schiller announced total cumulative sales of iPods exceeded 220 million. The continual decline of iPod sales since 2009 has not been a surprising trend for the Apple corporation, as Apple CFO Peter Oppenheimer explained in June 2009: \"We expect our traditional MP3 players to decline over time as we cannibalize ourselves with the iPod Touch and the iPhone.\" Since 2009, the company's iPod sales have continually decreased every financial quarter and in 2013 a new model was not introduced onto the market.\n",
      "\n",
      "Question: Who was Chief Financial Officer of Apple in July of 2009?\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prompt: {prompt}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Decoding Time: 2.7167 seconds\n",
      "Generated Text (Standard):\n",
      "Context: On October 21, 2008, Apple reported that only 14.21% of total revenue for fiscal quarter 4 of year 2008 came from iPods. At the September 9, 2009 keynote presentation at the Apple Event, Phil Schiller announced total cumulative sales of iPods exceeded 220 million. The continual decline of iPod sales since 2009 has not been a surprising trend for the Apple corporation, as Apple CFO Peter Oppenheimer explained in June 2009: \"We expect our traditional MP3 players to decline over time as we cannibalize ourselves with the iPod Touch and the iPhone.\" Since 2009, the company's iPod sales have continually decreased every financial quarter and in 2013 a new model was not introduced onto the market.\n",
      "\n",
      "Question: Who was Chief Financial Officer of Apple in July of 2009?\n",
      "\n",
      "Answer: Peter Oppenheimer\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speculative Decoding Time: 21.3564 seconds\n",
      "Generated Text (Speculative):\n",
      "Context: On October 21, 2008, Apple reported that only 14.21% of total revenue for fiscal quarter 4 of year 2008 came from iPods. At the September 9, 2009 keynote presentation at the Apple Event, Phil Schiller announced total cumulative sales of iPods exceeded 220 million. The continual decline of iPod sales since 2009 has not been a surprising trend for the Apple corporation, as Apple CFO Peter Oppenheimer explained in June 2009: \"We expect our traditional MP3 players to decline over time as we cannibalize ourselves with the iPod Touch and the iPhone.\" Since 2009, the company's iPod sales have continually decreased every financial quarter and in 2013 a new model was not introduced onto the market.\n",
      "\n",
      "Question: Who was Chief Financial Officer of Apple in July of 2009?\n",
      "\n",
      "Answer:Context: On October 21, 2008, Apple reported that only 14.21% of total revenue for fiscal quarter 4 of year 2008 came from iPods. At the September 9, 2009 keynote presentation at the Apple Event, Phil Schiller announced total cumulative sales of iPods exceeded 220 million. The continual decline of iPod sales since 2009 has not been a surprising trend for the Apple corporation, as Apple CFO Peter Oppenheimer explained in June 2009: \"We expect our traditional MP3 players to decline over time as we cannibalize ourselves with the iPod Touch and the iPhone.\" Since 2009, the company's iPod sales have continually decreased every financial quarter and in 2013 a new model was not introduced onto the market.\n",
      "\n",
      "Question: Who was Chief Financial Officer of Apple in July of 2009?\n",
      "\n",
      "Answer: Steve Wozniak. Most of the money flowed to Mark Wozniak, a leading board member of the company, since he acquired Apple in December 1980. The company's CFO, at the time, was Mark Wozniak. \n",
      "\n",
      "Question: Who was Chief Financial Officer of Apple in July of 2009?\n",
      "\n",
      "Answer: Mark Wozniak. Most of the money flowed to Mark Wozniak, a leading board member of the company, since he acquired Apple in December 1980. The company's CFO, at the time, was Mark Wozniak. \n",
      "\n",
      "Question: Who was Chief Financial Officer of Apple in July of 2009?\n",
      "\n",
      "Answer: Mark Wozniak. Most of the money flowed to Mark Wozniak,\n",
      "\n",
      "Speculative decoding improved inference time by -686.12%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Run standard decoding (large model only)\n",
    "generated_standard, time_standard = standard_decoding(prompt)\n",
    "print(f\"Standard Decoding Time: {time_standard:.4f} seconds\")\n",
    "print(f\"Generated Text (Standard):\\n{generated_standard}\\n\")\n",
    "\n",
    "# Run speculative decoding (small model + large model verification)\n",
    "generated_speculative, time_speculative = speculative_decoding(prompt)\n",
    "print(f\"Speculative Decoding Time: {time_speculative:.4f} seconds\")\n",
    "print(f\"Generated Text (Speculative):\\n{generated_speculative}\\n\")\n",
    "\n",
    "# Compare time improvements\n",
    "improvement = ((time_standard - time_speculative) / time_standard) * 100\n",
    "print(f\"Speculative decoding improved inference time by {improvement:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
