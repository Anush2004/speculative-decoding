{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anush2004/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load speculative (faster) model and verification (slower) model\n",
    "speculative_model_name = \"gpt2\"  # Faster model\n",
    "verification_model_name = \"meta-llama/Llama-3.2-1B-Instruct\"  # Accurate model\n",
    "\n",
    "speculative_tokenizer = AutoTokenizer.from_pretrained(speculative_model_name)\n",
    "speculative_model = AutoModelForCausalLM.from_pretrained(speculative_model_name)\n",
    "\n",
    "verification_tokenizer = AutoTokenizer.from_pretrained(verification_model_name)\n",
    "verification_model = AutoModelForCausalLM.from_pretrained(verification_model_name)\n",
    "\n",
    "# Move models to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "speculative_model.to(device)\n",
    "verification_model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "7\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "16\n",
      "18\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "23\n",
      "23\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "27\n",
      "28\n",
      "29\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "34\n",
      "35\n",
      "35\n",
      "36\n",
      "36\n",
      "36\n",
      "36\n",
      "38\n",
      "39\n",
      "41\n",
      "44\n",
      "Generated text: I am Neil Armstrong and I am going to the Moon in October 2015.\", Armstrong wrote in an email to the audience.\n",
      "\n",
      "Barry Matea\n",
      "\n",
      "Astronomer but has other wishes\n",
      "\n",
      "To Tim Omars, Jeffrey Bosch wrote a letter to Armstrong\n",
      "\n",
      "Rick Perry informs him of news[...]\n",
      "\n",
      "Welcome Ranger Magnum, Eddie. My name is Eddie Rone & myself as Jim Shannon, Dave Humancol, Greg Rosenhaug brown and Mike Flintfield, you are my name is Rick Rampone Tim Rone, out with dad. I'm meetup now & dad will be meeting with you there........✂ Facepalm. Go to Hedda's. Are you In. Rupert Murdoch & the Australian Times?hl Have I said I don't want to see or have I been hit badscar homicide and China theft? Shirt Bangers ...\" I will defend you again and I also need you to share with SDG during your absence we will meet at 8:00 p.m. I'm so worried about your scenario.Any trouble could bring you down & as I assume for name of my fingers I was gonna hit my back or they'd take you off the vent... I'm sorry but I must be so hopeless. …QI! all good. Top of h-Bbs Nat Frye I told him to tell you what this meeting will be like….Nothing what, will it be, what me?! That's what you pay my fries and Gardeld's a wee Victorian suburban woman and not care about sports ...let's all agree. Let us all be Free NC that's it to tell me my story night that party I'll never be due 4 fight way at Noon\n",
      "\n",
      "Travis Burnett informed I am Armstrong and said\n",
      "Acceptance ratio: 0.8800\n"
     ]
    }
   ],
   "source": [
    "def speculative_selection(speculative_probs, verification_probs):\n",
    "    # Find first index where verification probability is higher than speculative probability\n",
    "    first_ind = 0\n",
    "    break_flag = True\n",
    "    while(break_flag):\n",
    "        break_flag = False\n",
    "        for i in range(first_ind, len(speculative_probs)):\n",
    "            if verification_probs[i] < speculative_probs[i]:\n",
    "                first_ind = i\n",
    "                break_flag = True\n",
    "                break\n",
    "            \n",
    "        # Reject with probability 1 - verification_probs[first_ind]/speculative_probs[first_ind]\n",
    "        x = torch.rand(1)\n",
    "        if x > verification_probs[first_ind]/speculative_probs[first_ind]:\n",
    "            return first_ind\n",
    "\n",
    "    return len(verification_probs) - 1\n",
    "\n",
    "def return_speculative_tokens(speculative_probs, speculative_tokens, verification_probs, verification_tokens,accepted_token_count):\n",
    "    first_ind = speculative_selection(speculative_probs, verification_probs)\n",
    "    accepted_token_count += first_ind\n",
    "    print(accepted_token_count)\n",
    "    # print(verification_tokens.shape)\n",
    "    if(first_ind == len(verification_probs) - 1):\n",
    "        speculative_tokens = torch.cat([speculative_tokens, verification_tokens[first_ind]])\n",
    "    \n",
    "    else:\n",
    "        speculative_tokens = speculative_tokens[:first_ind + 1]\n",
    "        speculative_tokens[first_ind] = verification_tokens[first_ind]\n",
    "    \n",
    "    return speculative_tokens,accepted_token_count\n",
    "\n",
    "def speculative_decoding(prompt, max_length=50, speculative_steps=3):\n",
    "    # Tokenize input\n",
    "    speculative_inputs = speculative_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    verification_inputs = verification_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Initialize generated sequence with prompt tokens\n",
    "    generated_tokens = speculative_inputs['input_ids']\n",
    "    # print(generated_tokens)\n",
    "    # print(speculative_tokenizer.decode(generated_tokens[0]))\n",
    "    # Initialize counters for acceptance ratio calculation\n",
    "    speculative_token_count = 0\n",
    "    accepted_token_count = 0\n",
    "\n",
    "    # Start the time for speculative decoding\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Speculative model generates multiple tokens (speculative_steps)\n",
    "        spec_tokens = []\n",
    "        spec_probs = []\n",
    "        for _ in range(speculative_steps):\n",
    "            with torch.no_grad():\n",
    "                speculative_outputs = speculative_model(generated_tokens, return_dict=True)\n",
    "                speculative_logits = speculative_outputs.logits\n",
    "                # Use top-k sampling or any heuristic to select speculative tokens\n",
    "                speculative_probs = torch.softmax(speculative_logits[:, -1, :], dim=-1)\n",
    "                # next_speculative_tokens = torch.topk(speculative_probs, k=5, dim=-1).indices\n",
    "            \n",
    "                # next_speculative_prob = torch.topk(speculative_probs, k=5, dim=-1).values\n",
    "                # torch.multinomial\n",
    "                next_speculative_tokens = torch.multinomial(speculative_probs, num_samples=1)\n",
    "                next_speculative_prob = speculative_probs[0][next_speculative_tokens[0]]\n",
    "                spec_tokens.append(next_speculative_tokens[0])\n",
    "                spec_probs.append(next_speculative_prob[0])\n",
    "                generated_tokens = torch.cat([generated_tokens, next_speculative_tokens], dim=-1)\n",
    "                # print(generated_tokens)\n",
    "                # next_speculative_prob = torch.max(speculative_probs, dim=-1).values\n",
    "                # next_speculative_tokens = torch.argmax(speculative_probs)\n",
    "            # print(next_speculative_tokens, next_speculative_prob)\n",
    "                # print(speculative_tokenizer.decode(next_speculative_tokens[0], skip_special_tokens=True))\n",
    "            # Track speculative token count\n",
    "        # print(spec_tokens)\n",
    "        # print(spec_probs)\n",
    "        spec_tokens = torch.stack(spec_tokens).reshape(1,-1)\n",
    "        spec_probs = torch.stack(spec_probs).reshape(1,-1)\n",
    "        speculative_token_count += 1\n",
    "        # print(spec_tokens)\n",
    "        # print(verification_inputs['input_ids'])\n",
    "        new_verification_input = torch.cat([verification_inputs['input_ids'], spec_tokens[0].reshape(1,-1)], dim=-1)\n",
    "        # print(new_verification_input)\n",
    "        # print(new_verification_input.shape)\n",
    "        llm_output = verification_model(new_verification_input, return_dict=True)\n",
    "        verification_logits = llm_output.logits\n",
    "        # print(verification_logits.shape)\n",
    "        verification_probs = torch.softmax(verification_logits[:,-speculative_steps-1:,:], dim=-1)\n",
    "        # print(verification_probs.shape)\n",
    "        next_verification_tokens = torch.tensor([], dtype=torch.long)  \n",
    "        next_verification_prob = torch.tensor([], dtype=torch.float)   \n",
    "        for i in range(speculative_steps+1):\n",
    "            # Sample the next token from the multinomial distribution of the large model's verification probabilities\n",
    "            sampled_token = torch.multinomial(verification_probs[0, i], num_samples=1)\n",
    "\n",
    "            # Concatenate the newly sampled token with the previous tokens\n",
    "            next_verification_tokens = torch.cat((next_verification_tokens, sampled_token), dim=0) \n",
    "            \n",
    "            # Get the probability of the sampled token from the verification model's output\n",
    "            token_prob = verification_probs[0, i, sampled_token]\n",
    "            \n",
    "            # Concatenate the probability of the sampled token\n",
    "            next_verification_prob = torch.cat((next_verification_prob, token_prob), dim=0)\n",
    "\n",
    "        # Print shapes of the final tensors\n",
    "        # print(next_verification_tokens.shape)\n",
    "        # print(next_verification_prob.shape)\n",
    "        spec_tokens,accepted_token_count = return_speculative_tokens(spec_probs[0], spec_tokens[0], next_verification_prob, next_verification_tokens,accepted_token_count)\n",
    "\n",
    "    # End the time measurement for speculative decoding\n",
    "    # end_time = time.time()\n",
    "\n",
    "    # Calculate total time taken\n",
    "    # total_time_taken = end_time - start_time\n",
    "\n",
    "    # Calculate acceptance ratio\n",
    "    acceptance_ratio = accepted_token_count / speculative_token_count if speculative_token_count > 0 else 0\n",
    "\n",
    "    # Decode the generated tokens back to text\n",
    "    final_output = speculative_tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "    \n",
    "    return final_output, acceptance_ratio\n",
    "\n",
    "\n",
    "# Run speculative decoding\n",
    "prompt = \"I am Neil Armstrong and I am going to the\"\n",
    "generated_text, acceptance_ratio = speculative_decoding(prompt,speculative_steps=7)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Generated text: {generated_text}\")\n",
    "# print(f\"Time taken for speculative decoding: {time_taken:.4f} seconds\")\n",
    "print(f\"Acceptance ratio: {acceptance_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
