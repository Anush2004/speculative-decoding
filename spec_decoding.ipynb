{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anush2004/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load speculative (faster) model and verification (slower) model\n",
    "speculative_model_name = \"gpt2\"  # Faster model\n",
    "verification_model_name = \"meta-llama/Llama-3.2-1B-Instruct\"  # Accurate model\n",
    "\n",
    "speculative_tokenizer = AutoTokenizer.from_pretrained(speculative_model_name)\n",
    "speculative_model = AutoModelForCausalLM.from_pretrained(speculative_model_name)\n",
    "\n",
    "verification_tokenizer = AutoTokenizer.from_pretrained(verification_model_name)\n",
    "verification_model = AutoModelForCausalLM.from_pretrained(verification_model_name)\n",
    "\n",
    "# Move models to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "speculative_model.to(device)\n",
    "verification_model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## llm time taken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: I am Neil Armstrong and I am going to the moon. I have not seen anyone use 21 metres per second, I am 6'3\". When I tell people that I am yelling during the third aid, there is a psychological kick to take now. Why do we have to fight?\n",
      "To determine if being 'messy' or fit raises any questions, I scheduled a flight that was free. It was 10pm when I first learned yesterday that I had been told I was going to exhaustion by 24 hours with seven cigarette packs. Most of the passengers were sweating. The air is running out and you must keep turning to do the same. It took some looking. It was deathly cold, I allowed the most pleasant nights out and went home feeling totally at ease.\n",
      "I spoke with a gynecologist. He gave me water. It is because of the strength of the joints that I have an increase in my risk of anemia I would like to go back on my exercise after exercise. My training and recovery in previous X-2-2 puts me on a very low, laced diet. The same thing is happening for all of our other total body training. Almost every day here I am not up to decent enough for every kind of exercise â€” sketching, doing some leg\n",
      "Acceptance ratio: 0.8400\n",
      "Time taken for speculative decoding: 41.3197 seconds\n"
     ]
    }
   ],
   "source": [
    "def speculative_selection(speculative_probs, verification_probs):\n",
    "    # Find first index where verification probability is higher than speculative probability\n",
    "    first_ind = 0\n",
    "    break_flag = True\n",
    "    while(break_flag):\n",
    "        break_flag = False\n",
    "        for i in range(first_ind, len(speculative_probs)):\n",
    "            if verification_probs[i] < speculative_probs[i]:\n",
    "                first_ind = i\n",
    "                break_flag = True\n",
    "                break\n",
    "            \n",
    "        # Reject with probability 1 - verification_probs[first_ind]/speculative_probs[first_ind]\n",
    "        x = torch.rand(1)\n",
    "        if x > verification_probs[first_ind]/speculative_probs[first_ind]:\n",
    "            return first_ind\n",
    "\n",
    "    return len(verification_probs) - 1\n",
    "\n",
    "def return_speculative_tokens(speculative_probs, speculative_tokens, verification_probs, verification_tokens,accepted_token_count):\n",
    "    first_ind = speculative_selection(speculative_probs, verification_probs)\n",
    "    accepted_token_count += first_ind\n",
    "    # print(accepted_token_count)\n",
    "    # print(verification_tokens.shape)\n",
    "    if(first_ind == len(verification_probs) - 1):\n",
    "        speculative_tokens = torch.cat([speculative_tokens, verification_tokens[first_ind]])\n",
    "    \n",
    "    else:\n",
    "        speculative_tokens = speculative_tokens[:first_ind + 1]\n",
    "        speculative_tokens[first_ind] = verification_tokens[first_ind]\n",
    "    \n",
    "    return speculative_tokens,accepted_token_count\n",
    "\n",
    "def speculative_decoding(prompt, max_length=50, speculative_steps=3):\n",
    "    # Tokenize input\n",
    "    speculative_inputs = speculative_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    verification_inputs = verification_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Initialize generated sequence with prompt tokens\n",
    "    generated_tokens = speculative_inputs['input_ids']\n",
    "    # print(generated_tokens)\n",
    "    # print(speculative_tokenizer.decode(generated_tokens[0]))\n",
    "    # Initialize counters for acceptance ratio calculation\n",
    "    speculative_token_count = 0\n",
    "    accepted_token_count = 0\n",
    "\n",
    "    # Start the time for speculative decoding\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Speculative model generates multiple tokens (speculative_steps)\n",
    "        spec_tokens = []\n",
    "        spec_probs = []\n",
    "        for _ in range(speculative_steps):\n",
    "            with torch.no_grad():\n",
    "                speculative_outputs = speculative_model(generated_tokens, return_dict=True)\n",
    "                speculative_logits = speculative_outputs.logits\n",
    "                # Use top-k sampling or any heuristic to select speculative tokens\n",
    "                speculative_probs = torch.softmax(speculative_logits[:, -1, :], dim=-1)\n",
    "                # next_speculative_tokens = torch.topk(speculative_probs, k=5, dim=-1).indices\n",
    "            \n",
    "                # next_speculative_prob = torch.topk(speculative_probs, k=5, dim=-1).values\n",
    "                # torch.multinomial\n",
    "                next_speculative_tokens = torch.multinomial(speculative_probs, num_samples=1)\n",
    "                next_speculative_prob = speculative_probs[0][next_speculative_tokens[0]]\n",
    "                spec_tokens.append(next_speculative_tokens[0])\n",
    "                spec_probs.append(next_speculative_prob[0])\n",
    "                generated_tokens = torch.cat([generated_tokens, next_speculative_tokens], dim=-1)\n",
    "                # print(generated_tokens)\n",
    "                # next_speculative_prob = torch.max(speculative_probs, dim=-1).values\n",
    "                # next_speculative_tokens = torch.argmax(speculative_probs)\n",
    "            # print(next_speculative_tokens, next_speculative_prob)\n",
    "                # print(speculative_tokenizer.decode(next_speculative_tokens[0], skip_special_tokens=True))\n",
    "            # Track speculative token count\n",
    "        # print(spec_tokens)\n",
    "        # print(spec_probs)\n",
    "        spec_tokens = torch.stack(spec_tokens).reshape(1,-1)\n",
    "        spec_probs = torch.stack(spec_probs).reshape(1,-1)\n",
    "        speculative_token_count += 1\n",
    "        # print(spec_tokens)\n",
    "        # print(verification_inputs['input_ids'])\n",
    "        new_verification_input = torch.cat([verification_inputs['input_ids'], spec_tokens[0].reshape(1,-1)], dim=-1)\n",
    "        # print(new_verification_input)\n",
    "        # print(new_verification_input.shape)\n",
    "        llm_output = verification_model(new_verification_input, return_dict=True)\n",
    "        verification_logits = llm_output.logits\n",
    "        # print(verification_logits.shape)\n",
    "        verification_probs = torch.softmax(verification_logits[:,-speculative_steps-1:,:], dim=-1)\n",
    "        # print(verification_probs.shape)\n",
    "        next_verification_tokens = torch.tensor([], dtype=torch.long)  \n",
    "        next_verification_prob = torch.tensor([], dtype=torch.float)   \n",
    "        for i in range(speculative_steps+1):\n",
    "            # Sample the next token from the multinomial distribution of the large model's verification probabilities\n",
    "            sampled_token = torch.multinomial(verification_probs[0, i], num_samples=1)\n",
    "\n",
    "            # Concatenate the newly sampled token with the previous tokens\n",
    "            next_verification_tokens = torch.cat((next_verification_tokens, sampled_token), dim=0) \n",
    "            \n",
    "            # Get the probability of the sampled token from the verification model's output\n",
    "            token_prob = verification_probs[0, i, sampled_token]\n",
    "            \n",
    "            # Concatenate the probability of the sampled token\n",
    "            next_verification_prob = torch.cat((next_verification_prob, token_prob), dim=0)\n",
    "\n",
    "        # Print shapes of the final tensors\n",
    "        # print(next_verification_tokens.shape)\n",
    "        # print(next_verification_prob.shape)\n",
    "        spec_tokens,accepted_token_count = return_speculative_tokens(spec_probs[0], spec_tokens[0], next_verification_prob, next_verification_tokens,accepted_token_count)\n",
    "\n",
    "    # End the time measurement for speculative decoding\n",
    "    # end_time = time.time()\n",
    "\n",
    "    # Calculate total time taken\n",
    "    # total_time_taken = end_time - start_time\n",
    "\n",
    "    # Calculate acceptance ratio\n",
    "    acceptance_ratio = accepted_token_count / speculative_token_count if speculative_token_count > 0 else 0\n",
    "\n",
    "    # Decode the generated tokens back to text\n",
    "    final_output = speculative_tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "    return final_output, time_taken,acceptance_ratio\n",
    "\n",
    "\n",
    "\n",
    "# Run speculative decoding\n",
    "prompt = \"I am Neil Armstrong and I am going to the\"\n",
    "generated_text, time_taken,acceptance_ratio = speculative_decoding(prompt,speculative_steps=5)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Generated text: {generated_text}\")\n",
    "# print(f\"Time taken for speculative decoding: {time_taken:.4f} seconds\")\n",
    "print(f\"Acceptance ratio: {acceptance_ratio:.4f}\")\n",
    "print(f\"Time taken for speculative decoding: {time_taken:.4f} seconds\")    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for verification decoding: 20.8701 seconds\n",
      "I am Neil Armstrong and I am going to the moon!\n",
      "I'm not exactly sure how I'm going to get to the moon, but I know I'm going to do it. And when I do, I'm going to make the first human stands Privete PI Republic evaluating NY nationwide europe greetings\n"
     ]
    }
   ],
   "source": [
    "verification_inputs = verification_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "start_time = time.time()\n",
    "for i in range(50): \n",
    "    llm_output = verification_model(verification_inputs['input_ids'], return_dict=True)\n",
    "    \n",
    "    verification_logits = llm_output.logits\n",
    "    verification_probs = torch.softmax(verification_logits[:, -1, :], dim=-1)\n",
    "    next_verification_token = torch.multinomial(verification_probs, num_samples=1)\n",
    "    verification_inputs['input_ids'] = torch.cat([verification_inputs['input_ids'], next_verification_token], dim=-1)\n",
    "    \n",
    "    # print(verification_tokenizer.decode(next_verification_token[0], skip_special_tokens=True))\n",
    "    # print(verification_inputs['input_ids'])\n",
    "    \n",
    "    # print(verification_tokenizer.decode(verification_inputs['input_ids'][0], skip_special_tokens=True))\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "print(f\"Time taken for verification decoding: {time_taken:.4f} seconds\")\n",
    "print(verification_tokenizer.decode(verification_inputs['input_ids'][0], skip_special_tokens=True))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
