{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 12:19:43.389204: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-14 12:19:43.421964: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-14 12:19:43.421999: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-14 12:19:43.422809: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-14 12:19:43.428254: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-14 12:19:44.473231: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d16ed7a79848c0aed5d97b00640464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We will use the Google Llama-3.2 3B Instruct as the model we want to accelerate (3B parameters)\n",
    "target_model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "target = AutoModelForCausalLM.from_pretrained(target_model_name)\n",
    "\n",
    "# We will use the Google Llama-3.2 1B Instruct as the drafter model (1B parameters)\n",
    "drafter_model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "drafter = AutoModelForCausalLM.from_pretrained(drafter_model_name)\n",
    "\n",
    "# Don't forget to load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(target_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Translate to English: Je m'appelle Romain. N'hésitez pas à contribuer à mon projet !\"\n",
    "\n",
    "chat_templated = f\"<bos><start_of_turn>user\\n{prefix}<end_of_turn>\\n<start_of_turn>model\\n\" # Gemma chat template\n",
    "input_ids = tokenizer(chat_templated, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids[0].tolist() # Generation methods require a list of ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.logits_processor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbase_decoding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  autoregressive_generate\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspeculative_decoding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m speculative_generate\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogits_processors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NucleusProcessor\n",
      "File \u001b[0;32m~/ANLP/Project/speculative-decoding/v2.0/base_decoding.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogits_processor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogitsProcessor, GreedyProcessor\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[1;32m      8\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mautoregressive_generate\u001b[39m(\n\u001b[1;32m     10\u001b[0m     inputs: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     use_cache: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.logits_processor'"
     ]
    }
   ],
   "source": [
    "from base_decoding import  autoregressive_generate\n",
    "from speculative_decoding import speculative_generate\n",
    "from utils.logits_processors import NucleusProcessor\n",
    "\n",
    "# Parameters\n",
    "gen_len = 100       # Maximum number of tokens generated (could over pass when using speculative decoding)\n",
    "gamma = 4           # Number of drafts generated by the drafter model at each step\n",
    "logits_processor = NucleusProcessor(temperature=.6, top_p=.9) # Nucleus sampling with p=0.9 and T=0.6\n",
    "\n",
    "# Generate text using the classic auto-regressive decoding (slow)\n",
    "output_ids_ar = autoregressive_generate( # or autoregressive_generate_encoder_decoder for encoder-decoder models\n",
    "                input_ids,\n",
    "                target,\n",
    "                logits_processor=logits_processor,\n",
    "                max_gen_len=gen_len,\n",
    "                eos_tokens_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "output_ar = tokenizer.decode(output_ids_ar, skip_special_tokens=True)\n",
    "\n",
    "# Generate text using the speculative decoding (faster)\n",
    "output_ids_sd, alpha = speculative_generate( # or speculative_generate_encoder_decoder for encoder-decoder models\n",
    "                input_ids,\n",
    "                drafter,\n",
    "                target,\n",
    "                logits_processor=logits_processor,\n",
    "                gamma=gamma,\n",
    "                max_gen_len=gen_len,\n",
    "                eos_tokens_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "output_sd = tokenizer.decode(output_ids_sd, skip_special_tokens=True)\n",
    "\n",
    "print(\"Auto-regressive decoding:\", output_ar)\n",
    "print(\"Speculative decoding:\", output_sd)\n",
    "print(\"Acceptance rate:\", alpha) # Number of drafts accepted by the target model divided by the number of drafts generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "torch.Size([90000000]) torch.Size([5000000])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created\n",
      "tensor([116, 101, 114,  93,  93,  32, 110,  97, 109, 101, 100,  32,  91,  91,\n",
      "         65, 114, 105, 117, 115,  93,  93,  32,  98, 101, 103,  97, 110,  32,\n",
      "        116, 101,  97,  99, 104, 105, 110, 103,  32, 116, 104,  97, 116,  32,\n",
      "        116, 104, 101, 114, 101,  32, 119,  97, 115,  32,  97,  32, 116, 105,\n",
      "        109, 101,  32,  98, 101, 102, 111, 114, 101,  32,  91,  91,  71, 111,\n",
      "        100,  93,  93,  32, 116, 104, 101,  32,  70,  97, 116, 104, 101, 114,\n",
      "         32,  98, 101, 103,  97, 116,  32,  91,  91,  74, 101, 115, 117, 115,\n",
      "         93,  93,  32, 119, 104, 101, 110,  32, 116, 104, 101,  32, 108,  97,\n",
      "        116, 116, 101, 114,  32, 100, 105, 100,  32, 110, 111, 116,  32, 101,\n",
      "        120, 105, 115, 116,  46,  32,  65, 116, 104,  97, 110,  97, 115, 105,\n",
      "        117, 115,  32,  97,  99,  99, 111, 109, 112,  97, 110, 105, 101, 100,\n",
      "         32,  91,  91,  65, 108, 101, 120,  97, 110, 100, 101, 114,  32, 111,\n",
      "        102,  32,  65, 108, 101, 120,  97, 110, 100, 114, 105,  97, 124,  65,\n",
      "        108, 101, 120,  97, 110, 100, 101, 114,  93,  93,  32, 116, 111,  32,\n",
      "        116, 104, 101,  32,  91,  91,  70, 105, 114, 115, 116,  32,  67, 111,\n",
      "        117, 110,  99, 105, 108,  32, 111, 102,  32,  78, 105,  99,  97, 101,\n",
      "         97,  93,  93,  32, 105, 110,  32,  91,  91,  51,  50,  53,  93,  93,\n",
      "         44,  32, 119, 104, 105,  99, 104,  32,  99, 111, 117, 110,  99, 105,\n",
      "        108,  32, 112, 114, 111, 100, 117,  99, 101, 100,  32, 116, 104, 101,\n",
      "         32,  91,  91,  78, 105,  99, 101, 110, 101,  32,  67, 114, 101, 101,\n",
      "        100,  93,  93,  32,  97, 110, 100,  32,  97, 110,  97, 116, 104, 101,\n",
      "        109,  97, 116, 105, 122, 101, 100,  32,  65, 114, 105, 117, 115,  32,\n",
      "         97, 110, 100,  32, 104, 105, 115,  32, 102, 111, 108, 108, 111, 119,\n",
      "        101, 114, 115,  46,  32,  79, 110,  32,  91,  91,  77,  97, 121,  32,\n",
      "         57,  93,  93,  44,  32,  91,  91,  51,  50,  56,  93,  93,  44,  32,\n",
      "        104, 101,  32, 115, 117,  99,  99, 101, 101, 100, 101, 100,  32,  91,\n",
      "         91,  65, 108, 101, 120,  97, 110, 100, 101, 114,  32, 111, 102,  32,\n",
      "         65, 108, 101, 120,  97, 110, 100, 114, 105,  97, 124,  65, 108, 101,\n",
      "        120,  97, 110, 100, 101, 114,  93,  93,  32,  97, 115,  32,  98, 105,\n",
      "        115, 104, 111, 112,  32, 111, 102,  32,  91,  91,  65, 108, 101, 120,\n",
      "         97, 110, 100, 114, 105,  97,  93,  93,  46,  32,  65, 115,  32,  97,\n",
      "         32, 114, 101, 115, 117, 108, 116,  32, 111, 102,  32, 114, 105, 115,\n",
      "        101, 115,  32,  97, 110, 100,  32, 102,  97, 108, 108, 115,  32, 105,\n",
      "        110,  32,  65, 114, 105,  97, 110, 105, 115, 109,  39, 115,  32, 105,\n",
      "        110, 102, 108, 117, 101, 110,  99, 101,  44,  32, 104, 101,  32, 119,\n",
      "         97, 115,  32,  98,  97, 110, 105, 115, 104, 101, 100,  32, 102, 114,\n",
      "        111, 109,  32,  65, 108, 101, 120,  97, 110])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
