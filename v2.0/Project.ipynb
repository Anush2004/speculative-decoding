{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel,AutoModelForCausalLM\n",
    "import torch\n",
    "import gc\n",
    "import tqdm as tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM Head: Linear(in_features=2048, out_features=128256, bias=False)\n"
     ]
    }
   ],
   "source": [
    "dummy = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "lm_head = dummy.lm_head  # This is the layer responsible for mapping hidden states to vocab logits\n",
    "print(\"LM Head:\", lm_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import inspect\n",
    "# lines = inspect.getsource(model.forward)\n",
    "# print(lines)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Italy?\n",
      "What is the last answer?\n",
      "What is the capital of Italy?What is the last answer?\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "query1 = \"What is the capital of Italy?\"\n",
    "query2 = \"What is the last answer?\"\n",
    "query_size1 = len(tokenizer(query1)[\"input_ids\"])  # Token count for each query\n",
    "max_out1 = query_size1 + 1  # Maximum tokens including both query and generated response per query\n",
    "query_size2 = len(tokenizer(query2)[\"input_ids\"])  # Token count for each query\n",
    "max_out2 = query_size2 + 1  # Maximum tokens including both query and generated response per query\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# Pad input for each query and concatenate\n",
    "inputs1 = tokenizer(query1, padding='max_length', max_length=max_out1, return_tensors=\"pt\")\n",
    "inputs2 = tokenizer(query2, padding='max_length', max_length=max_out2, return_tensors=\"pt\")\n",
    "\n",
    "print(tokenizer.decode(inputs1[\"input_ids\"][0],skip_special_tokens=True))\n",
    "print(tokenizer.decode(inputs2[\"input_ids\"][0],skip_special_tokens=True))\n",
    "\n",
    "# Stack the input IDs with padding for each query's space\n",
    "input_ids = torch.cat([inputs1['input_ids'], inputs2['input_ids']], dim=1).to(device)\n",
    "print(tokenizer.decode(input_ids[0],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Build custom attention mask\n",
    "L = max_out1 + max_out2  # Total length (combined max output for both queries)\n",
    "attention_mask = torch.zeros((L, L), dtype=torch.float16)\n",
    "\n",
    "# Block-diagonal causal mask for each query and response output\n",
    "# First block for query 1\n",
    "attention_mask[:max_out1, :max_out1] = torch.tril(torch.ones((max_out1, max_out1), dtype=torch.float16))\n",
    "# Second block for query 2\n",
    "attention_mask[max_out1:, max_out1:] = torch.tril(torch.ones((max_out2, max_out2), dtype=torch.float16))\n",
    "# attention_mask = 1 - attention_mask\n",
    "\n",
    "print(attention_mask)\n",
    "# Expand to batch size and move to device\n",
    "attention_mask = attention_mask.unsqueeze(0).unsqueeze(0).to(device)\n",
    "# attention_mask = attention_mask.to(device)\n",
    "\n",
    "# Run inference with the custom attention mask\n",
    "with torch.no_grad():\n",
    "    outputs = model.forward(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "\n",
    "logits = lm_head(hidden_states)\n",
    "output_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(max_out1)\n",
    "\n",
    "# print(outputs.keys())\n",
    "# Decode and split output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([22463]), tensor([220])]\n"
     ]
    }
   ],
   "source": [
    "oid =  [output_ids[0][query_size1-1:max_out1-1], output_ids[0][max_out1+query_size2-1:-1]] \n",
    "print(oid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "full_response = tokenizer.decode(oid[1], skip_special_tokens=True)\n",
    "# response1 = full_response[:max_out].strip()  # Output for query 1\n",
    "# response2 = full_response[max_out:].strip()  # Output for query 2\n",
    "\n",
    "# print(\"Response to Query 1:\", response1)\n",
    "# print(\"Response to Query 2:\", response2)\n",
    "print(full_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
