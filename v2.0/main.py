# %%
from transformers import AutoTokenizer, AutoModelForCausalLM
import time
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")
# %%
# We will use the Google Llama-3.2 3B Instruct as the model we want to accelerate (3B parameters)
target_model_name = "openai-community/gpt2-xl"
target = AutoModelForCausalLM.from_pretrained(target_model_name).to(device)
# openai-community/gpt2-medium
# We will use the Google Llama-3.2 1B Instruct as the drafter model (1B parameters)
drafter_model_name = "openai-community/gpt2-medium"
drafter = AutoModelForCausalLM.from_pretrained(drafter_model_name).to(device)

# Don't forget to load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(target_model_name)

# %%
prefix = "So today started of with a haze all over the city, it seemed like the city was waking up from a long slumber. The streets were deserted, the only"
# sound was the chirping of the birds. I decided to go for a walk in the park, the park was empty, the grass was wet with dew. Suddenly, I saw a shadow moving in the bushes. I went closer to see what it was and I saw a little puppy. I decided to take it home with me. I named it Max. Max was a cute little puppy with big brown eyes. I decided to take care of him and make him a part of my family. I took him to the vet to get him checked and vaccinated. Max was a healthy puppy and he was very playful. I took him for walks in the park every day and he loved it. Max became my best friend and I was very happy to have him in my life."

# chat_templated = f"<bos><start_of_turn>user\n{prefix}<end_of_turn>\n<start_of_turn>model\n" # Gemma chat template
chat_templated = f"{prefix}"
input_ids = tokenizer(chat_templated, return_tensors="pt").input_ids
input_ids = input_ids[0].tolist() # Generation methods require a list of ids

# %%
from base_decoding import  autoregressive_generate
from speculative_decoding import speculative_generate
from utils.logits_processors import NucleusProcessor,GreedyProcessor
import time
# Parameters
gen_len = 100       # Maximum number of tokens generated (could over pass when using speculative decoding)
gamma = 10           # Number of drafts generated by the drafter model at each step
logits_processor = NucleusProcessor(temperature=.6, top_p=.9) # Nucleus sampling with p=0.9 and T=0.6
# logits_processor = GreedyProcessor(temperature=0.1) # Greedy decoding
tokenizer.pad_token_id = tokenizer.eos_token_id # We use the eos token as the pad token

# Generate text using the classic auto-regressive decoding (slow)
start = time.time()
output_ids_ar = autoregressive_generate( 
                input_ids,
                target,
                logits_processor=logits_processor,
                max_gen_len=gen_len,
                eos_tokens_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.pad_token_id,
                use_cache=False,
            )
end = time.time()
original_time = end - start
print(f"Time for auto-regressive decoding:{end - start} seconds")
output_ar = tokenizer.decode(output_ids_ar, skip_special_tokens=True)

# Generate text using the speculative decoding (faster)
start = time.time()
output_ids_sd, alpha = speculative_generate( 
                input_ids,
                drafter,
                target,
                logits_processor=logits_processor,
                gamma=gamma,
                max_gen_len=gen_len,
                eos_tokens_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.pad_token_id,
                use_cache=False,

            )
end = time.time()
print(f"Time for speculative decoding:{end - start} seconds")
output_sd = tokenizer.decode(output_ids_sd, skip_special_tokens=True)

print("Auto-regressive decoding:", prefix + output_ar)
print("Speculative decoding:",  prefix + output_sd)
print("Acceptance rate:", alpha) # Number of drafts accepted by the target model divided by the number of drafts generated

# %%
print(f"Speed up: {original_time/(end-start)}x ")

# %%


# %%


# %%


# %%


# %%


# %%


# %%



